<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <script src="https://www.kmaasrud.com/web-components/head.js"></script>
    <script src="https://www.kmaasrud.com/web-components/katex.js"></script>
    <script src="https://www.kmaasrud.com/web-components/highlight.js"></script>
    <title>brain/Bayesian information criterion</title>
</head>
<body>
    <div id="main">
        <div id="header">brain/<strong>Bayesian information criterion</strong></div>
        <script src="https://www.kmaasrud.com/web-components/header.js"></script>
        <div id="content"><div id="content"><p>The <strong>Bayesian information criterion</strong> (BIC) or <strong>Schwarz information criterion</strong> (SIC) is a criterion for <a href="model-selectionmodel-selection">model selection</a> in a finite set of models. We prefer the model with the lowest BIC. Its general defenition is</p>

<p>$$ \text{BIC} = \log N\cdot d - 2\ \text{loglik} ,$$</p>

<p>where $N$ is the number of data points $d$ is the number of parameters estimated by the model and $\text{loglik}$ is the log of the maximized <a href="likelihood-functionlikelihood-function">likelihood function</a>.</p>

<p>Sources:</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">Wikipedia</a></li>
<li><a href="the-elements-of-statistical-learning">The Elements of Statistical Learning</a></li>
</ul>
</div><div class="backlinks">

<ul>
<li><a href="stk-in4300-lecture-02-linear-methods-for-regression">STK-IN4300 Lecture 02 - Linear methods for regression</a></li>
</ul>

</div>
</div>
    </div>
    <script src="https://www.kmaasrud.com/web-components/darkModeToggle.js"></script>
</body>
</html>
