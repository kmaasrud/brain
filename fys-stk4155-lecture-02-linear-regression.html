<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <script src="https://www.kmaasrud.com/web-components/head.js"></script>
    <script src="https://www.kmaasrud.com/web-components/katex.js"></script>
    <script src="https://www.kmaasrud.com/web-components/highlight.js"></script>
    <title>brain/FYS-STK4155 Lecture 02 - Linear regression</title>
</head>
<body>
    <div id="main">
        <div id="header">brain/<strong>FYS-STK4155 Lecture 02 - Linear regression</strong></div>
        <script src="https://www.kmaasrud.com/web-components/header.js"></script>
        <div id="content"><div id="content"><h2 id="preliminary">Preliminary</h2>

<p>(<strong>Note</strong>: This lecture uses indices starting at $0$)</p>

<p>(We will normally split our data in training data and test data)</p>

<p>Recall: <a href="machine-learning">Machine learning</a> has three important elements:</p>

<ul>
<li>The data set</li>
<li>The model</li>
<li>The cost/error/loss function</li>
</ul>

<h2 id="linear-regressionlinear-regression"><a href="linear-regression">Linear regression</a></h2>

<h3 id="data-set">Data set</h3>

<p>We have a data set, which contains some variables $(y,x)$. $x$ we call the <em>input</em> and $y$ we call the <em>output</em> or <em>target</em>. The general assumption we make is:</p>

<p>$$ y(x) = f(x) + N(0, \sigma^2) ,$$</p>

<p>where $N$ is a <a href="probability-density-functionpdf">PDF</a> (<a href="normal-distribution">Normal distribution</a>), where $\mu = 0$ and we have a variance $\sigma^2$. Then we assume this <a href="probability-density-functionpdf">PDF</a> $P(x)$ is known and <a href="continuitycontinuous">continuous</a>, then our mean value is defined:</p>

<p>$$\sigma = \int dx x P(x),$$</p>

<p>However, in our case, $P(x)$ is often unkown. <a href="machine-learning">Machine learning</a> is a <a href="frequentist-approachfrequentist-approach">frequentist approach</a> to this.</p>

<p>$$ (y,x)\rightarrow x= \begin{matrix}\{x_0,x_1,...,x_{n-1}\} \ \{y_0,y_1,...,y_{n-1}\}\end{matrix} $$</p>

<p>The mean value we estimate:</p>

<p>$$ \mu_y = \frac{1}{n}\sum_{i=0}^{n-1}y_i $$<br />
$$ \mu_y \ne \mu $$</p>

<p>So $\mu_y$ is not the exact mean, but the <strong><a href="sample-meansample-mean">sample mean</a></strong>.</p>

<hr />

<p>$$ \sigma^2 = \int (x-\mu)^2 P(x) dx ,$$</p>

<p>the true <a href="variance">Variance</a>. However, we have to live with the <strong><a href="sample-variancesample-variance">sample variance</a></strong>.</p>

<p>$$ \sigma_x^2 = \frac{1}{n}\sum_{i=0}^{n-1}(x_i \mu_x)^2 $$<br />
$$\sigma_x^2 \ne \sigma^2$$</p>

<p>(when <a href="morten-hjorth-jensenmorten">Morten</a> puts a subscript to the variance or mean, he is talking about the sampled version)</p>

<hr />

<p>So now back to the function $f(x)$, which is the function we seek. It can be approximated</p>

<p>$$ f(x) \simeq \tilde y(x) $$</p>

<p>We discretize</p>

<p>$$ y(x) \rightarrow y(x_i) = y_i $$<br />
$$ y(x_i) \simeq \tilde y_i $$<br />
$$ \epsilon_i \sim N(0,\sigma^2) $$</p>

<p>Now we need to make a <strong>model</strong></p>

<h3 id="model">Model</h3>

<p>We have some data points, which we hope to match linearly</p>

<p>$$ \tilde y_i = \tilde y_i(x_i) = \beta_0 + \beta_1 x_i $$<br />
$$ \mathbf{\beta} = \{\beta_0, \beta_1\} $$</p>

<p>How can we find $\beta$, so that the distance between $y$ (target) and $\tilde y$ is as small as possible?</p>

<h3 id="cost-function">Cost function</h3>

<p>(When you see $y - \tilde y$, this equals $\sum_{i=0}^{n-1} (y_i - \tilde y_i$)</p>

<p>We have the <a href="mean-squared-errormean-squared-error-mse">mean squared error (MSE)</a>:</p>

<p>$$ \text{MSE} = \frac{1}{n}\sum_{i=0}^{n-1}(y_i - \tilde y_i) = C(\mathbf{\beta}|x) $$</p>

<p>or <a href="cost-function">Cost function</a> (which <a href="morten-hjorth-jensenmorten">Morten</a> will write as $C(\mathbf{\beta})$ from now on).</p>

<p>We seek an optimal $\hat \beta = \text{argmin}_{\beta} C(\mathbf{\beta}1|x)$.</p>

<p>We can rewrite the <a href="cost-function">Cost function</a> (<a href="morten-hjorth-jensenmortens">Morten's</a> slides of this <a href="https://compphysics.github.io/MachineLearning/doc/pub/Regression/html/Regression.html#___sec12">here</a>)</p>

<p>$$ C(\mathbf{\beta}) = \frac{1}{n}\left[\left(\mathbf y - \mathbf{\tilde y}\right)^T \left(\mathbf y -\mathbf{\tilde y}\right)\right] $$</p>

<p>We want to minimize the spread of $C(\beta)$, which in practical terms means we require:</p>

<p>$$ \frac{\partial C(\mathbf \beta)}{\partial \beta_j} = ... = 0 $$</p>

<p>$$ \frac{\partial C(\mathbf \beta)}{\partial \beta_j} = 0 = \mathbf X^T (\mathbf y - \mathbf X \mathbf \beta) $$</p>

<p>which can be rewritten into</p>

<p>$$ \mathbf X^T \mathbf y = \mathbf X^T \mathbf X \mathbf \beta .$$</p>

<p>If the matrix $\mathbf X^T \mathbf X$ is <a href="invertibilityinvertible">invertible</a>, we have our solution, namely:</p>

<p>$$ \mathbf \beta = \left(\mathbf X^T \mathbf X\right)^{-1} \mathbf X^T \mathbf y $$</p>

<h2 id="practical-application">Practical application</h2>

<p>From <code>sklearn.linear_model</code> we can import <code>LinearRegression</code> which contains the handy function <code>fit</code>.</p>

<p>(Hopefully, <a href="morten-hjorth-jensenmorten">Morten</a> will go more into this)</p>

<hr />

<p>Meta:</p>

<ul>
<li>Course: <a href="fys-stk4155">FYS-STK4155</a></li>
<li>Lecturer: <a href="morten-hjorth-jensen">Morten Hjorth-Jensen</a></li>
<li>Date: <a href="daily/2020-08-20">daily/2020-08-20</a></li>
</ul>
</div><div class="backlinks">

<ul>
<li><a href="testing-new-syntax">Testing new syntax</a></li>
</ul>

</div>
</div>
    </div>
    <script src="https://www.kmaasrud.com/web-components/darkModeToggle.js"></script>
</body>
</html>
